{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b30ae219",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T10:54:37.326841Z",
     "iopub.status.busy": "2025-12-28T10:54:37.326718Z",
     "iopub.status.idle": "2025-12-28T10:54:37.506548Z",
     "shell.execute_reply": "2025-12-28T10:54:37.506121Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb4cf6a",
   "metadata": {},
   "source": [
    "# 1. 实现金融领域的专有名词标准化系统\n",
    "Implement a Financial Domain Proper Noun Standardization System."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4d91c90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T10:54:37.507821Z",
     "iopub.status.busy": "2025-12-28T10:54:37.507712Z",
     "iopub.status.idle": "2025-12-28T10:54:38.566430Z",
     "shell.execute_reply": "2025-12-28T10:54:38.566040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 15724 terms from data/万条金融标准术语.csv\n",
      "Standardization Results:\n",
      "Original: I went to icbc to deposit money.\n",
      "Standardized: I went to Industrial and Commercial Bank of China to Deposit Money.\n",
      "--------------------\n",
      "Original: Using alipay is convenient.\n",
      "Standardized: Using Alipay is convenient.\n",
      "--------------------\n",
      "Original: 工行 is a large bank.\n",
      "Standardized: Industrial and Commercial Bank of China is a large Bank.\n",
      "--------------------\n",
      "Original: The a priori probability is high.\n",
      "Standardized: The A Priori Probability is high.\n",
      "--------------------\n",
      "Original: we need a round financing.\n",
      "Standardized: we need A Round Financing.\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "class FinancialTermStandardizer:\n",
    "    def __init__(self, term_mapping: Dict[str, str] = None):\n",
    "        self.term_mapping = term_mapping or {}\n",
    "        # Sort keys by length (descending) to match longest terms first\n",
    "        self.sorted_keys = sorted(self.term_mapping.keys(), key=len, reverse=True)\n",
    "\n",
    "    def add_term(self, alias: str, canonical: str):\n",
    "        self.term_mapping[alias] = canonical\n",
    "        self.sorted_keys = sorted(self.term_mapping.keys(), key=len, reverse=True)\n",
    "\n",
    "    def standardize(self, text: str) -> str:\n",
    "        # A simple replacement strategy. \n",
    "        # Using regex with word boundaries to prevent partial matches (e.g. 'POS' inside 'Deposit')\n",
    "        result = text\n",
    "        for alias in self.sorted_keys:\n",
    "            # Skip single characters to avoid excessive false positives (like 'a', 'i')\n",
    "            if len(alias) < 2:\n",
    "                continue\n",
    "                \n",
    "            # Use word boundaries \\b and case-insensitive matching\n",
    "            pattern = r'\\b' + re.escape(alias) + r'\\b'\n",
    "            if re.search(pattern, result, re.IGNORECASE):\n",
    "                result = re.sub(pattern, self.term_mapping[alias], result, flags=re.IGNORECASE)\n",
    "        return result\n",
    "\n",
    "# Load mappings from CSV\n",
    "csv_path = 'data/万条金融标准术语.csv'\n",
    "mapping = {}\n",
    "\n",
    "if os.path.exists(csv_path):\n",
    "    try:\n",
    "        # Based on file inspection, columns are 'A' (Term) and 'FINTERM' (Type)\n",
    "        df_terms = pd.read_csv(csv_path)\n",
    "        if 'A' in df_terms.columns:\n",
    "            terms = df_terms['A'].dropna().unique().tolist()\n",
    "            for term in terms:\n",
    "                if isinstance(term, str):\n",
    "                    # Strategy: Map lowercase variant to standard Term\n",
    "                    mapping[term.lower()] = term\n",
    "            print(f\"Loaded {len(mapping)} terms from {csv_path}\")\n",
    "        else:\n",
    "            print(\"Column 'A' not found in CSV. Checking first column.\")\n",
    "            terms = df_terms.iloc[:, 0].dropna().unique().tolist()\n",
    "            for term in terms:\n",
    "                 if isinstance(term, str):\n",
    "                    mapping[term.lower()] = term\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CSV: {e}\")\n",
    "else:\n",
    "    print(f\"Warning: {csv_path} not found. Using empty mapping.\")\n",
    "\n",
    "# Add some manual overrides if needed\n",
    "manual_mapping = {\n",
    "    '工行': 'Industrial and Commercial Bank of China',\n",
    "    '蚂蚁金服': 'Ant Group',\n",
    "    'ICBC': 'Industrial and Commercial Bank of China',\n",
    "    'ALIPAY': 'Alipay'\n",
    "}\n",
    "for k, v in manual_mapping.items():\n",
    "    mapping[k.lower()] = v\n",
    "    mapping[k] = v # Also add original case for manual ones\n",
    "\n",
    "standardizer = FinancialTermStandardizer(mapping)\n",
    "\n",
    "# Test\n",
    "test_sentences = [\n",
    "    \"I went to icbc to deposit money.\",\n",
    "    \"Using alipay is convenient.\",\n",
    "    \"工行 is a large bank.\",\n",
    "    \"The a priori probability is high.\", # Should capitalize 'A Priori Probability'\n",
    "    \"we need a round financing.\" # Should capitalize 'A Round Financing'\n",
    "]\n",
    "\n",
    "print(\"Standardization Results:\")\n",
    "for sent in test_sentences:\n",
    "    print(f\"Original: {sent}\")\n",
    "    print(f\"Standardized: {standardizer.standardize(sent)}\")\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0366ac",
   "metadata": {},
   "source": [
    "# 2. 自学习并实践不同的数据导入方法\n",
    "Self-study and practice different data import methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb369ca3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T10:54:38.567673Z",
     "iopub.status.busy": "2025-12-28T10:54:38.567604Z",
     "iopub.status.idle": "2025-12-28T10:54:38.573987Z",
     "shell.execute_reply": "2025-12-28T10:54:38.573599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TXT Import ---\n",
      "This is a sample text file for the RAG homework.\n",
      "It contains some financial terms like ICBC and ALIP\n",
      "\n",
      "--- CSV Import ---\n",
      "   id                content    source\n",
      "0   1  Financial Report 2023  Internal\n",
      "1   2     Market Analysis Q1  External\n",
      "\n",
      "--- PDF Import ---\n",
      "Financial Report Overview\n",
      "This document contains financial data.\n",
      "The table below shows the revenue:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ensure data exists\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "    # (Assuming data generation logic is handled externally or here if needed)\n",
    "    # For this notebook execution, we assume data/sample.txt, csv, pdf exist.\n",
    "\n",
    "def read_text_file(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "def read_csv_file(path):\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def read_pdf_file(path):\n",
    "    text_content = []\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text_content.append(page.extract_text())\n",
    "    return \"\\n\".join(text_content)\n",
    "\n",
    "print(\"--- TXT Import ---\")\n",
    "try:\n",
    "    print(read_text_file('data/sample.txt')[:100])\n",
    "except Exception as e: print(e)\n",
    "\n",
    "print(\"\\n--- CSV Import ---\")\n",
    "try:\n",
    "    print(read_csv_file('data/sample.csv').head())\n",
    "except Exception as e: print(e)\n",
    "\n",
    "print(\"\\n--- PDF Import ---\")\n",
    "try:\n",
    "    print(read_pdf_file('data/sample.pdf')[:100])\n",
    "except Exception as e: print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c25f0f1",
   "metadata": {},
   "source": [
    "# 3. 重构 Load File, Chunk File, Parse File\n",
    "Refactor Load File, Chunk File, and Parse File."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93fb416f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T10:54:38.574928Z",
     "iopub.status.busy": "2025-12-28T10:54:38.574865Z",
     "iopub.status.idle": "2025-12-28T10:54:38.577290Z",
     "shell.execute_reply": "2025-12-28T10:54:38.576949Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader initialized.\n"
     ]
    }
   ],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def load(self, file_path: str, file_type: str = None) -> str:\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "        \n",
    "        if file_type is None:\n",
    "            file_type = file_path.split('.')[-1].lower()\n",
    "        \n",
    "        if file_type == 'txt':\n",
    "            return self._load_txt(file_path)\n",
    "        elif file_type == 'csv':\n",
    "            return self._load_csv(file_path)\n",
    "        elif file_type == 'pdf':\n",
    "            return self._load_pdf(file_path)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type: {file_type}\")\n",
    "\n",
    "    def _load_txt(self, path):\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "\n",
    "    def _load_csv(self, path):\n",
    "        # Convert CSV to string representation for RAG\n",
    "        df = pd.read_csv(path)\n",
    "        return df.to_json(orient='records', indent=2)\n",
    "\n",
    "    def _load_pdf(self, path):\n",
    "        # Use the parser for PDF\n",
    "        return PDFParser.parse(path)\n",
    "\n",
    "loader = DataLoader()\n",
    "print(\"DataLoader initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9a8259e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T10:54:38.578211Z",
     "iopub.status.busy": "2025-12-28T10:54:38.578147Z",
     "iopub.status.idle": "2025-12-28T10:54:38.580104Z",
     "shell.execute_reply": "2025-12-28T10:54:38.579807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDFParser defined.\n"
     ]
    }
   ],
   "source": [
    "class PDFParser:\n",
    "    @staticmethod\n",
    "    def parse(file_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Parses PDF, extracting text and tables.\n",
    "        Returns a JSON string containing text and metadata.\n",
    "        \"\"\"\n",
    "        result = {\n",
    "            \"file_path\": file_path,\n",
    "            \"pages\": []\n",
    "        }\n",
    "        \n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            for i, page in enumerate(pdf.pages):\n",
    "                page_data = {\n",
    "                    \"page_number\": i + 1,\n",
    "                    \"text\": page.extract_text() or \"\",\n",
    "                    \"tables\": page.extract_tables(),\n",
    "                    \"images_metadata\": page.images  # Metadata about images (pos, size)\n",
    "                }\n",
    "                result[\"pages\"].append(page_data)\n",
    "                \n",
    "        return json.dumps(result, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"PDFParser defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb6a5ce6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T10:54:38.581005Z",
     "iopub.status.busy": "2025-12-28T10:54:38.580943Z",
     "iopub.status.idle": "2025-12-28T10:54:38.583958Z",
     "shell.execute_reply": "2025-12-28T10:54:38.583603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextChunker defined.\n"
     ]
    }
   ],
   "source": [
    "class TextChunker:\n",
    "    def __init__(self, chunk_size=100, overlap=20):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.overlap = overlap\n",
    "\n",
    "    def chunk(self, text: str, method='fixed') -> List[Dict[str, Any]]:\n",
    "        if method == 'fixed':\n",
    "            return self._chunk_fixed(text)\n",
    "        elif method == 'recursive':\n",
    "            return self._chunk_recursive(text)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown chunking method\")\n",
    "\n",
    "    def _chunk_fixed(self, text: str) -> List[Dict[str, Any]]:\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        text_len = len(text)\n",
    "        \n",
    "        while start < text_len:\n",
    "            end = min(start + self.chunk_size, text_len)\n",
    "            chunk_text = text[start:end]\n",
    "            chunks.append({\n",
    "                \"content\": chunk_text,\n",
    "                \"metadata\": {\"start\": start, \"end\": end, \"method\": \"fixed\"}\n",
    "            })\n",
    "            start += (self.chunk_size - self.overlap)\n",
    "        return chunks\n",
    "\n",
    "    def _chunk_recursive(self, text: str) -> List[Dict[str, Any]]:\n",
    "        # Simple simulation of recursive chunking by splitting on newlines/sentences\n",
    "        # In a real scenario, use langchain's RecursiveCharacterTextSplitter\n",
    "        separators = [\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "        # This is a simplified version just splitting by newline for demonstration\n",
    "        parts = text.split('\\n')\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        for part in parts:\n",
    "            if len(current_chunk) + len(part) < self.chunk_size:\n",
    "                current_chunk += part + \"\\n\"\n",
    "            else:\n",
    "                if current_chunk:\n",
    "                    chunks.append({\n",
    "                        \"content\": current_chunk.strip(),\n",
    "                        \"metadata\": {\"method\": \"recursive\"}\n",
    "                    })\n",
    "                current_chunk = part + \"\\n\"\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append({\n",
    "                \"content\": current_chunk.strip(),\n",
    "                \"metadata\": {\"method\": \"recursive\"}\n",
    "            })\n",
    "            \n",
    "        return chunks\n",
    "\n",
    "chunker = TextChunker(chunk_size=50, overlap=10)\n",
    "print(\"TextChunker defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfea3710",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T10:54:38.584838Z",
     "iopub.status.busy": "2025-12-28T10:54:38.584779Z",
     "iopub.status.idle": "2025-12-28T10:54:38.806767Z",
     "shell.execute_reply": "2025-12-28T10:54:38.806438Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Integrated Pipeline Test ---\n",
      "Data Loaded (First 200 chars): {\n",
      "  \"file_path\": \"data/sample.pdf\",\n",
      "  \"pages\": [\n",
      "    {\n",
      "      \"page_number\": 1,\n",
      "      \"text\": \"Financial Report Overview\\nThis document contains financial data.\\nThe table below shows the revenue:\\nYea\n",
      "\n",
      "Standardized Text (Snippet): Financial Report Overview\n",
      "This document contains financial data.\n",
      "The table below shows the Revenue:\n",
      "\n",
      "\n",
      "Generated 4 chunks using fixed method.\n",
      "Chunk 1: {'content': 'Financial Report Overview\\nThis document contains f', 'metadata': {'start': 0, 'end': 50, 'method': 'fixed'}}\n",
      "Chunks saved to data/output_chunks.json\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Integrated Pipeline Test ---\")\n",
    "\n",
    "# 1. Load PDF (uses Parser internally)\n",
    "try:\n",
    "    loaded_data = loader.load('data/sample.pdf', file_type='pdf')\n",
    "    print(\"Data Loaded (First 200 chars):\", loaded_data[:200])\n",
    "    \n",
    "    # Parse the JSON string back to dict to access text\n",
    "    data_dict = json.loads(loaded_data)\n",
    "    full_text = \"\"\n",
    "    for page in data_dict['pages']:\n",
    "        full_text += page['text'] + \"\\n\"\n",
    "        if page['tables']:\n",
    "            print(f\"Found {len(page['tables'])} tables on page {page['page_number']}\")\n",
    "            print(\"Table 1:\", page['tables'][0])\n",
    "\n",
    "    # 2. Standardize Text\n",
    "    # Add some terms found in the text if any, or just demonstrate on sample text\n",
    "    std_text = standardizer.standardize(full_text)\n",
    "    print(\"\\nStandardized Text (Snippet):\", std_text[:100])\n",
    "\n",
    "    # 3. Chunk Text\n",
    "    chunks = chunker.chunk(std_text, method='fixed')\n",
    "    print(f\"\\nGenerated {len(chunks)} chunks using fixed method.\")\n",
    "    print(\"Chunk 1:\", chunks[0])\n",
    "    \n",
    "    # Save chunks to JSON\n",
    "    with open('data/output_chunks.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(chunks, f, ensure_ascii=False, indent=2)\n",
    "    print(\"Chunks saved to data/output_chunks.json\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error in integrated test:\", e)\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
