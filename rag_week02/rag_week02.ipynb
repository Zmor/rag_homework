{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG框架作业实现\n",
    "## 集成私有Embedding模型与Chroma向量数据库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 环境准备与依赖安装\n",
    "\n",
    "在开始之前，请确保已安装所有必要的依赖项。您可以使用以下命令创建虚拟环境并安装依赖：\n",
    "\n",
    "```bash\n",
    "conda create -n rag_week02 python=3.9\n",
    "conda activate rag_week02\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 核心模块定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import chromadb\n",
    "\n",
    "class CustomEmbedding:\n",
    "    def __init__(self, api_key, base_url, model_name):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = base_url\n",
    "        self.model_name = model_name\n",
    "        self.client = OpenAI(api_key=self.api_key, base_url=self.base_url)\n",
    "    \n",
    "    def get_embeddings(self, texts: list[str]) -> list[list[float]]:\n",
    "        try:\n",
    "            response = self.client.embeddings.create(\n",
    "                input=texts,\n",
    "                model=self.model_name\n",
    "            )\n",
    "            embeddings = [item.embedding for item in response.data]\n",
    "            return embeddings\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting embeddings: {e}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChromaDBManager:\n",
    "    def __init__(self, collection_name: str, embedding_function):\n",
    "        self.client = chromadb.Client()\n",
    "        self.collection_name = collection_name\n",
    "        self.collection = self.client.get_or_create_collection(\n",
    "            name=collection_name,\n",
    "            embedding_function=embedding_function\n",
    "        )\n",
    "    \n",
    "    def add_documents(self, documents: list[str], metadatas: list[dict], ids: list[str]):\n",
    "        self.collection.add(\n",
    "            documents=documents,\n",
    "            metadatas=metadatas,\n",
    "            ids=ids\n",
    "        )\n",
    "    \n",
    "    def query(self, query_text: str, n_results: int = 5) -> list[dict]:\n",
    "        results = self.collection.query(\n",
    "            query_texts=[query_text],\n",
    "            n_results=n_results\n",
    "        )\n",
    "        \n",
    "        formatted_results = []\n",
    "        for i in range(len(results['ids'][0])):\n",
    "            formatted_results.append({\n",
    "                \"document\": results['documents'][0][i],\n",
    "                \"metadata\": results['metadatas'][0][i],\n",
    "                \"distance\": results['distances'][0][i]\n",
    "            })\n",
    "        \n",
    "        return formatted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomReranker:\n",
    "    def __init__(self, api_key, base_url, model_name):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = base_url\n",
    "        self.model_name = model_name\n",
    "    \n",
    "    def rerank(self, query: str, documents: list[str], top_n: int = 3) -> list[dict]:\n",
    "        try:\n",
    "            headers = {\n",
    "                \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            }\n",
    "            \n",
    "            payload = {\n",
    "                \"model\": self.model_name,\n",
    "                \"query\": query,\n",
    "                \"passages\": documents\n",
    "            }\n",
    "            \n",
    "            response = requests.post(\n",
    "                f\"{self.base_url}/rerank\",\n",
    "                headers=headers,\n",
    "                json=payload\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                results = response.json()[\"results\"]\n",
    "                # Sort by relevance score and take top_n\n",
    "                sorted_results = sorted(results, key=lambda x: x[\"relevance_score\"], reverse=True)[:top_n]\n",
    "                return [{\"document\": documents[item[\"index\"]], \"relevance_score\": item[\"relevance_score\"]} \n",
    "                        for item in sorted_results]\n",
    "            else:\n",
    "                print(f\"Rerank API request failed with status code: {response.status_code}\")\n",
    "                return []\n",
    "        except Exception as e:\n",
    "            print(f\"Error during reranking: {e}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLLM:\n",
    "    def __init__(self, api_key, base_url, model_name):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = base_url\n",
    "        self.model_name = model_name\n",
    "        self.client = OpenAI(api_key=self.api_key, base_url=self.base_url)\n",
    "    \n",
    "    def generate(self, prompt: str) -> str:\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating response: {e}\")\n",
    "            return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RAG框架整合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "\n",
    "class SimpleRAG:\n",
    "    def __init__(self):\n",
    "        # API配置\n",
    "        self.embedding_api_key = os.getenv('EMBEDDING_API_KEY', '')\n",
    "        self.embedding_base_url = os.getenv('EMBEDDING_BASE_URL', '/api/inference/v1')\n",
    "        self.embedding_model_name = os.getenv('EMBEDDING_MODEL_NAME', 'bge-large-zh-v1.5')\n",
    "        \n",
    "        self.rerank_api_key = os.getenv('RERANKER_API_KEY', '')\n",
    "        self.rerank_base_url = os.getenv('RERANKER_BASE_URL', '/api/inference/v1')\n",
    "        self.rerank_model_name = os.getenv('RERANKER_MODEL_NAME', 'bge-reranker-v2-m3')\n",
    "        \n",
    "        self.llm_api_key = os.getenv('LLM_API_KEY', '')\n",
    "        self.llm_base_url = os.getenv('LLM_BASE_URL', '/api/inference/v1')\n",
    "        self.llm_model_name = os.getenv('LLM_MODEL_NAME', 'GLM-4.6-FP8')\n",
    "        \n",
    "        # 初始化模块\n",
    "        self.embedding_client = CustomEmbedding(\n",
    "            self.embedding_api_key, \n",
    "            self.embedding_base_url, \n",
    "            self.embedding_model_name\n",
    "        )\n",
    "        \n",
    "        # 注意：ChromaDBManager需要一个embedding函数，这里我们传递get_embeddings方法\n",
    "        self.db_manager = ChromaDBManager(\n",
    "            \"rag_collection\", \n",
    "            self.embedding_client.get_embeddings\n",
    "        )\n",
    "        \n",
    "        self.reranker = CustomReranker(\n",
    "            self.rerank_api_key, \n",
    "            self.rerank_base_url, \n",
    "            self.rerank_model_name\n",
    "        )\n",
    "        \n",
    "        self.llm_client = CustomLLM(\n",
    "            self.llm_api_key, \n",
    "            self.llm_base_url, \n",
    "            self.llm_model_name\n",
    "        )\n",
    "    \n",
    "    def ingest(self, documents: list[str], metadatas: list[dict] = None, ids: list[str] = None):\n",
    "        # 自动生成metadatas和ids（如果未提供）\n",
    "        if metadatas is None:\n",
    "            metadatas = [{\"source\": \"default\"} for _ in documents]\n",
    "        \n",
    "        if ids is None:\n",
    "            ids = [str(uuid.uuid4()) for _ in documents]\n",
    "        \n",
    "        # 获取文档的embedding\n",
    "        embeddings = self.embedding_client.get_embeddings(documents)\n",
    "        \n",
    "        # 将文档和其向量存入Chroma\n",
    "        self.db_manager.add_documents(documents, metadatas, ids)\n",
    "    \n",
    "    def query(self, question: str, use_rerank: bool = True) -> dict:\n",
    "        # 步骤1：获取问题的embedding\n",
    "        question_embedding = self.embedding_client.get_embeddings(<question>)\n",
    "        \n",
    "        # 步骤2：检索相关文档\n",
    "        retrieved_docs = self.db_manager.query(question, n_results=5)\n",
    "        \n",
    "        # 步骤3：如果use_rerank为True，则进行重排序\n",
    "        if use_rerank and retrieved_docs:\n",
    "            doc_texts = [doc[\"document\"] for doc in retrieved_docs]\n",
    "            reranked_docs = self.reranker.rerank(question, doc_texts, top_n=3)\n",
    "            context = \"\\n\".join([doc[\"document\"] for doc in reranked_docs])\n",
    "        else:\n",
    "            context = \"\\n\".join([doc[\"document\"] for doc in retrieved_docs[:3]])\n",
    "        \n",
    "        # 步骤4：构建提示词模板\n",
    "        prompt = f\"\"\"基于以下上下文回答问题：\n",
    "\n",
    "上下文：\n",
    "{context}\n",
    "\n",
    "问题：{question}\n",
    "\n",
    "请根据上下文回答问题。如果上下文中没有相关信息，请说明无法基于提供的上下文回答问题。\"\"\"\n",
    "        \n",
    "        # 步骤5：生成最终答案\n",
    "        answer = self.llm_client.generate(prompt)\n",
    "        \n",
    "        # 步骤6：返回结构化结果\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"context\": context,\n",
    "            \"answer\": answer\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 端到端演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义示例文档数据\n",
    "documents = [\n",
    "    \"人工智能（Artificial Intelligence，AI）是指由人类制造出来的机器所表现出来的智能。通常人工智能是指通过普通计算机程序来呈现人类智能的技术。\",\n",
    "    \"机器学习是人工智能的一个分支，它使计算机能够从数据中学习并做出决策或预测，而无需明确编程来执行特定任务。\",\n",
    "    \"深度学习是机器学习的一个子集，它模仿人脑的工作方式，使用神经网络来处理和学习复杂的数据模式。深度学习在图像识别、语音识别和自然语言处理等领域取得了显著成果。\",\n",
    "    \"自然语言处理（NLP）是计算机科学和人工智能领域的一个重要方向，它致力于让计算机理解和生成人类语言。\",\n",
    "    \"计算机视觉是一门研究如何让计算机'看'的科学，它赋予了计算机理解和解释图像和视频内容的能力。\"\n",
    "]\n",
    "\n",
    "# 实例化SimpleRAG类\n",
    "rag = SimpleRAG()\n",
    "\n",
    "# 调用ingest方法，将示例文档数据加载到RAG系统中\n",
    "rag.ingest(documents)\n",
    "\n",
    "# 定义示例问题\n",
    "question = \"请解释一下什么是深度学习，它与机器学习有什么关系？\"\n",
    "\n",
    "# 调用query方法，并打印返回的结果\n",
    "result = rag.query(question)\n",
    "\n",
    "print(f\"问题：{result['question']}\")\n",
    "print(f\"\\n检索到的上下文：\\n{result['context']}\")\n",
    "print(f\"\\n生成的答案：\\n{result['answer']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
